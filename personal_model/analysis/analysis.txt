BPM - beats per minute, tempo
Chroma STFT - chromagram from waveform/power spectrogram, represents info about classification of pitch and signal structure
Spectral centroid - measure of the amplitude at the center of the spectrum of the signal distribution over a window calculated from the Fourier transform frequency and amplitude information
Zero Crossing Rate (zcr) - rate that signal changes from positive to negative

Background info:

- audio visualization library with javascript
- talk music theory 
- tried training my own model but discovered the difficulty of curating a reliable dataset, training cost, RAM/GPU

It started with trying to train a binary classification model to see whether or not a particular song was predicted to fall into the genre Drum and Bass.
I curated a small dataset of songs and extracted features using a Python audio library called Librosa, then formatted it into a csv file to train my Tensorflow model.
It was just a basic Sequential Convolution Neural Network, but I quickly realized it was a lot more complex than I had intially believed it to be. 
One of the obstacles I encountered was that with Drum and Bass, the drum kick is emphasized on certain beats, and this sometimes tricked the beat-per-minute predictor into thinking
the song was going at half-tempo rather than normal time. For example, if a song was going at 180 bpm, sometimes the feauture extractor would label it as 90 bpm which would be too low 
to classify as the genre.

Due to this, I pivoted to using open-source genre prediction models by Essentia, and this had much better results than my own model.
I used Flask to create a basic web app that would run my Python backend whenever a user uploaded an mp3 file they wanted to analyze.